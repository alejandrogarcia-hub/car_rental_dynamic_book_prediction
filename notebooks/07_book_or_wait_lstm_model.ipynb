{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book or Wait: LSTM Model Implementation\n",
    "\n",
    "This notebook implements an LSTM (Long Short-Term Memory) neural network for the \"Book or Wait\" decision system.\n",
    "LSTMs excel at learning sequential patterns in time series data, making them ideal for capturing temporal price dynamics.\n",
    "\n",
    "## Key Features:\n",
    "- Sequential price history modeling (14-day lookback window)\n",
    "- Combined sequence and static feature processing\n",
    "- CPU/GPU compatible PyTorch implementation\n",
    "- Advanced temporal feature engineering\n",
    "- Early stopping and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device for CPU/GPU compatibility\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load synthetic data\n",
    "data_dir = Path('../data/synthetic_data')\n",
    "\n",
    "print(\"Loading synthetic data...\")\n",
    "users_df = pd.read_csv(data_dir / 'synthetic_users.csv')\n",
    "searches_df = pd.read_csv(data_dir / 'synthetic_searches.csv')\n",
    "bookings_df = pd.read_csv(data_dir / 'synthetic_bookings.csv')\n",
    "rental_prices_df = pd.read_csv(data_dir / 'synthetic_rental_prices.csv')\n",
    "competitor_prices_df = pd.read_csv(data_dir / 'synthetic_competitor_prices.csv')\n",
    "\n",
    "# Convert timestamps\n",
    "searches_df['search_ts'] = pd.to_datetime(searches_df['search_ts'])\n",
    "bookings_df['booking_ts'] = pd.to_datetime(bookings_df['booking_ts'])\n",
    "rental_prices_df['date'] = pd.to_datetime(rental_prices_df['obs_ts'])\n",
    "competitor_prices_df['date'] = pd.to_datetime(competitor_prices_df['obs_date'])\n",
    "\n",
    "print(f\"Users: {users_df.shape}\")\n",
    "print(f\"Searches: {searches_df.shape}\")\n",
    "print(f\"Bookings: {bookings_df.shape}\")\n",
    "print(f\"Rental Prices: {rental_prices_df.shape}\")\n",
    "print(f\"Competitor Prices: {competitor_prices_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sequential Feature Engineering\n",
    "\n",
    "For LSTM models, we need to create sequences of historical price data rather than single-point features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(rental_prices_df, competitor_prices_df, sequence_length=14):\n",
    "    \"\"\"Create sequences of price data for LSTM training.\"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    static_features = []\n",
    "    metadata = []\n",
    "    \n",
    "    # Get unique combinations\n",
    "    price_keys = rental_prices_df[['supplier_id', 'location_id', 'car_class']].drop_duplicates()\n",
    "    \n",
    "    print(f\"Processing {len(price_keys)} unique supplier/location/class combinations...\")\n",
    "    \n",
    "    for idx, (_, key) in enumerate(price_keys.iterrows()):\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"Progress: {idx}/{len(price_keys)} combinations processed\")\n",
    "            \n",
    "        supplier_id = key['supplier_id']\n",
    "        location_id = key['location_id']\n",
    "        car_class = key['car_class']\n",
    "        \n",
    "        # Get price history\n",
    "        mask = (rental_prices_df['supplier_id'] == supplier_id) & \\\n",
    "               (rental_prices_df['location_id'] == location_id) & \\\n",
    "               (rental_prices_df['car_class'] == car_class)\n",
    "        \n",
    "        price_history = rental_prices_df[mask].sort_values('date')\n",
    "        \n",
    "        # Need enough data for sequences\n",
    "        if len(price_history) < sequence_length + 7:\n",
    "            continue\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(sequence_length, len(price_history) - 7):\n",
    "            current_date = price_history.iloc[i]['date']\n",
    "            current_price = price_history.iloc[i]['current_price']\n",
    "            \n",
    "            # Future prices for target\n",
    "            future_prices = price_history.iloc[i+1:i+8]['current_price'].values\n",
    "            max_future_price = np.max(future_prices)\n",
    "            should_book = 1 if current_price < max_future_price else 0\n",
    "            \n",
    "            # Create sequence features (price history)\n",
    "            sequence_data = []\n",
    "            for j in range(i - sequence_length, i):\n",
    "                row = price_history.iloc[j]\n",
    "                # Sequence features: price, availability, days until pickup, price change\n",
    "                seq_features = [\n",
    "                    row['current_price'],\n",
    "                    row['available_cars'],\n",
    "                    row['days_until_pickup']\n",
    "                ]\n",
    "                \n",
    "                # Add price changes if we have enough history\n",
    "                if j > 0:\n",
    "                    prev_price = price_history.iloc[j-1]['current_price']\n",
    "                    price_change = (row['current_price'] - prev_price) / prev_price\n",
    "                    seq_features.append(price_change)\n",
    "                else:\n",
    "                    seq_features.append(0.0)\n",
    "                \n",
    "                sequence_data.append(seq_features)\n",
    "            \n",
    "            # Static features (unchanging characteristics)\n",
    "            static_feature_vec = [\n",
    "                supplier_id,\n",
    "                location_id,\n",
    "                current_date.dayofweek,\n",
    "                current_date.month,\n",
    "                1 if current_date.dayofweek >= 5 else 0,  # is_weekend\n",
    "                1 if current_date.month in [6, 7, 8, 12] else 0,  # is_peak_season\n",
    "                (current_date.month - 1) // 3 + 1,  # quarter\n",
    "            ]\n",
    "            \n",
    "            # Add competitor features\n",
    "            comp_mask = (competitor_prices_df['location_id'] == location_id) & \\\n",
    "                       (competitor_prices_df['car_class'] == car_class) & \\\n",
    "                       (competitor_prices_df['date'] == current_date)\n",
    "            \n",
    "            comp_prices = competitor_prices_df[comp_mask]['comp_min_price'].values\n",
    "            if len(comp_prices) > 0:\n",
    "                static_feature_vec.extend([\n",
    "                    (current_price - np.mean(comp_prices)) / np.mean(comp_prices),  # price_vs_competitors\n",
    "                    1 if current_price < np.min(comp_prices) else 0,  # is_cheapest\n",
    "                    np.sum(current_price >= comp_prices) / len(comp_prices)  # price_rank\n",
    "                ])\n",
    "            else:\n",
    "                static_feature_vec.extend([0, 0, 0.5])\n",
    "            \n",
    "            sequences.append(sequence_data)\n",
    "            targets.append(should_book)\n",
    "            static_features.append(static_feature_vec)\n",
    "            \n",
    "            # Store metadata for analysis\n",
    "            metadata.append({\n",
    "                'date': current_date,\n",
    "                'supplier_id': supplier_id,\n",
    "                'location_id': location_id,\n",
    "                'car_class': car_class,\n",
    "                'current_price': current_price\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nCompleted processing. Created {len(sequences)} sequences.\")\n",
    "    return (np.array(sequences), np.array(targets), \n",
    "            np.array(static_features), pd.DataFrame(metadata))\n",
    "\n",
    "# Create sequences\n",
    "print(\"Creating price sequences...\")\n",
    "sequence_length = 14\n",
    "sequences, targets, static_features, metadata_df = create_sequences(\n",
    "    rental_prices_df, competitor_prices_df, sequence_length\n",
    ")\n",
    "\n",
    "print(f\"\\nSequence creation summary:\")\n",
    "print(f\"Total sequences: {len(sequences)}\")\n",
    "print(f\"Sequence shape: {sequences.shape}\")\n",
    "print(f\"Static features shape: {static_features.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(f\"  Wait (0): {(targets == 0).sum()} ({(targets == 0).mean():.1%})\")\n",
    "print(f\"  Book (1): {(targets == 1).sum()} ({(targets == 1).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch Dataset for Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceSequenceDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for sequential price data.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, targets, static_features=None, scaler=None, sequence_scaler=None):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.static_features = static_features\n",
    "        self.scaler = scaler\n",
    "        self.sequence_scaler = sequence_scaler\n",
    "        \n",
    "        # Scale static features\n",
    "        if static_features is not None:\n",
    "            if self.scaler is None:\n",
    "                self.scaler = StandardScaler()\n",
    "                self.static_scaled = self.scaler.fit_transform(static_features)\n",
    "            else:\n",
    "                self.static_scaled = self.scaler.transform(static_features)\n",
    "        else:\n",
    "            self.static_scaled = None\n",
    "        \n",
    "        # Scale sequences\n",
    "        if self.sequence_scaler is None:\n",
    "            self.sequence_scaler = MinMaxScaler()\n",
    "            # Reshape for scaling: (samples * timesteps, features)\n",
    "            original_shape = sequences.shape\n",
    "            sequences_reshaped = sequences.reshape(-1, sequences.shape[-1])\n",
    "            sequences_scaled = self.sequence_scaler.fit_transform(sequences_reshaped)\n",
    "            self.sequences_scaled = sequences_scaled.reshape(original_shape)\n",
    "        else:\n",
    "            original_shape = sequences.shape\n",
    "            sequences_reshaped = sequences.reshape(-1, sequences.shape[-1])\n",
    "            sequences_scaled = self.sequence_scaler.transform(sequences_reshaped)\n",
    "            self.sequences_scaled = sequences_scaled.reshape(original_shape)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.sequences_tensor = torch.FloatTensor(self.sequences_scaled)\n",
    "        self.targets_tensor = torch.FloatTensor(targets)\n",
    "        \n",
    "        if self.static_scaled is not None:\n",
    "            self.static_tensor = torch.FloatTensor(self.static_scaled)\n",
    "        else:\n",
    "            self.static_tensor = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.static_tensor is not None:\n",
    "            return self.sequences_tensor[idx], self.static_tensor[idx], self.targets_tensor[idx]\n",
    "        else:\n",
    "            return self.sequences_tensor[idx], self.targets_tensor[idx]\n",
    "\n",
    "# Time-based train/test split\n",
    "split_date = metadata_df['date'].quantile(0.8)\n",
    "train_mask = metadata_df['date'] < split_date\n",
    "test_mask = ~train_mask\n",
    "\n",
    "X_seq_train = sequences[train_mask]\n",
    "X_static_train = static_features[train_mask]\n",
    "y_train = targets[train_mask]\n",
    "\n",
    "X_seq_test = sequences[test_mask]\n",
    "X_static_test = static_features[test_mask]\n",
    "y_test = targets[test_mask]\n",
    "\n",
    "print(f\"Train sequences: {len(X_seq_train)}\")\n",
    "print(f\"Test sequences: {len(X_seq_test)}\")\n",
    "print(f\"Train target distribution: Wait={np.sum(y_train == 0)}, Book={np.sum(y_train == 1)}\")\n",
    "print(f\"Test target distribution: Wait={np.sum(y_test == 0)}, Book={np.sum(y_test == 1)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = PriceSequenceDataset(X_seq_train, y_train, X_static_train)\n",
    "test_dataset = PriceSequenceDataset(\n",
    "    X_seq_test, y_test, X_static_test,\n",
    "    scaler=train_dataset.scaler,\n",
    "    sequence_scaler=train_dataset.sequence_scaler\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBookOrWaitModel(nn.Module):\n",
    "    \"\"\"LSTM model with static features for Book or Wait prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_input_size, static_input_size=0, hidden_size=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMBookOrWaitModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.static_input_size = static_input_size\n",
    "        \n",
    "        # LSTM for sequential data\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=sequence_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        fc_input_size = hidden_size + static_input_size\n",
    "        self.fc1 = nn.Linear(fc_input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, sequence_input, static_input=None):\n",
    "        batch_size = sequence_input.size(0)\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(sequence_input.device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(sequence_input.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, _ = self.lstm(sequence_input, (h0, c0))\n",
    "        \n",
    "        # Take the last output\n",
    "        lstm_last = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Combine with static features if available\n",
    "        if static_input is not None and self.static_input_size > 0:\n",
    "            combined = torch.cat((lstm_last, static_input), dim=1)\n",
    "        else:\n",
    "            combined = lstm_last\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "sequence_input_size = sequences.shape[2]  # Number of features per timestep\n",
    "static_input_size = static_features.shape[1]  # Number of static features\n",
    "\n",
    "model = LSTMBookOrWaitModel(\n",
    "    sequence_input_size=sequence_input_size,\n",
    "    static_input_size=static_input_size,\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"Sequence input size: {sequence_input_size} features per timestep\")\n",
    "print(f\"Static input size: {static_input_size} features\")\n",
    "print(f\"Hidden size: 64\")\n",
    "print(f\"Number of LSTM layers: 2\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nModel summary:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device, has_static_features=True):\n",
    "    \"\"\"Train model for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_data in train_loader:\n",
    "        if has_static_features:\n",
    "            batch_sequences, batch_static, batch_targets = batch_data\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_static = batch_static.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_sequences, batch_static).squeeze()\n",
    "        else:\n",
    "            batch_sequences, batch_targets = batch_data\n",
    "            batch_sequences = batch_sequences.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_sequences).squeeze()\n",
    "        \n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total_loss += loss.item()\n",
    "        correct += (predicted == batch_targets).sum().item()\n",
    "        total += batch_targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device, has_static_features=True):\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            if has_static_features:\n",
    "                batch_sequences, batch_static, batch_targets = batch_data\n",
    "                batch_sequences = batch_sequences.to(device)\n",
    "                batch_static = batch_static.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(batch_sequences, batch_static).squeeze()\n",
    "            else:\n",
    "                batch_sequences, batch_targets = batch_data\n",
    "                batch_sequences = batch_sequences.to(device)\n",
    "                batch_targets = batch_targets.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(batch_sequences).squeeze()\n",
    "            \n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_loss += loss.item()\n",
    "            correct += (predicted == batch_targets).sum().item()\n",
    "            total += batch_targets.size(0)\n",
    "            \n",
    "            # Store for metrics\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(batch_targets.cpu().numpy())\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, np.array(all_predictions), np.array(all_targets), np.array(all_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Loss function: Binary Cross Entropy\")\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "test_aucs = []\n",
    "\n",
    "best_auc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    test_loss, test_acc, _, _, test_probs = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Calculate AUC\n",
    "    test_auc = roc_auc_score(y_test, test_probs)\n",
    "    \n",
    "    # Store history\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "    test_aucs.append(test_auc)\n",
    "    \n",
    "    # Save best model\n",
    "    if test_auc > best_auc:\n",
    "        best_auc = test_auc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1:2d}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test AUC: {test_auc:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nLoaded best model with AUC: {best_auc:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(test_losses, label='Test Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# AUC plot\n",
    "ax3.plot(test_aucs, label='Test AUC', color='green')\n",
    "ax3.axhline(y=best_auc, color='red', linestyle='--', label=f'Best AUC: {best_auc:.4f}')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('AUC')\n",
    "ax3.set_title('Test AUC Over Time')\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "# Learning curves comparison\n",
    "ax4.plot(train_losses, label='Train Loss', alpha=0.7)\n",
    "ax4.plot(test_losses, label='Test Loss', alpha=0.7)\n",
    "ax4_twin = ax4.twinx()\n",
    "ax4_twin.plot(test_aucs, color='green', label='Test AUC', linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Loss')\n",
    "ax4_twin.set_ylabel('AUC')\n",
    "ax4.set_title('Loss vs AUC Progression')\n",
    "ax4.legend(loc='upper right')\n",
    "ax4_twin.legend(loc='lower right')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation\n",
    "print(\"LSTM Model Evaluation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "_, _, predictions, targets_final, probabilities = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(targets_final, predictions, target_names=['Wait', 'Book Now']))\n",
    "\n",
    "# ROC-AUC Score\n",
    "final_auc = roc_auc_score(targets_final, probabilities)\n",
    "print(f\"\\nFinal ROC-AUC Score: {final_auc:.4f}\")\n",
    "\n",
    "# Additional metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, support = precision_recall_fscore_support(targets_final, predictions)\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"Wait class - Precision: {precision[0]:.3f}, Recall: {recall[0]:.3f}, F1: {f1[0]:.3f}\")\n",
    "print(f\"Book class - Precision: {precision[1]:.3f}, Recall: {recall[1]:.3f}, F1: {f1[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Confusion Matrix and ROC Curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(targets_final, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Wait', 'Book Now'], \n",
    "            yticklabels=['Wait', 'Book Now'], ax=ax1)\n",
    "ax1.set_title('LSTM Model - Confusion Matrix')\n",
    "ax1.set_ylabel('True Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(targets_final, probabilities)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {final_auc:.2f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax2.set_xlim([0.0, 1.0])\n",
    "ax2.set_ylim([0.0, 1.05])\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('LSTM Model - ROC Curve')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Limitations Analysis\n",
    "\n",
    "Let's analyze why the LSTM model underperformed compared to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data availability for sequence creation\n",
    "print(\"Data Availability Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check how many combinations have sufficient data\n",
    "price_keys = rental_prices_df[['supplier_id', 'location_id', 'car_class']].drop_duplicates()\n",
    "sufficient_data = 0\n",
    "insufficient_data = 0\n",
    "data_lengths = []\n",
    "\n",
    "for _, key in price_keys.iterrows():\n",
    "    mask = (rental_prices_df['supplier_id'] == key['supplier_id']) & \\\n",
    "           (rental_prices_df['location_id'] == key['location_id']) & \\\n",
    "           (rental_prices_df['car_class'] == key['car_class'])\n",
    "    \n",
    "    price_history = rental_prices_df[mask]\n",
    "    data_lengths.append(len(price_history))\n",
    "    \n",
    "    if len(price_history) >= sequence_length + 7:\n",
    "        sufficient_data += 1\n",
    "    else:\n",
    "        insufficient_data += 1\n",
    "\n",
    "print(f\"Total combinations: {len(price_keys)}\")\n",
    "print(f\"Sufficient data (≥{sequence_length + 7} days): {sufficient_data} ({sufficient_data/len(price_keys)*100:.1f}%)\")\n",
    "print(f\"Insufficient data: {insufficient_data} ({insufficient_data/len(price_keys)*100:.1f}%)\")\n",
    "print(f\"Average data length: {np.mean(data_lengths):.1f} days\")\n",
    "print(f\"Median data length: {np.median(data_lengths):.1f} days\")\n",
    "\n",
    "# Plot data length distribution\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=sequence_length + 7, color='red', linestyle='--', \n",
    "            label=f'Min required: {sequence_length + 7} days')\n",
    "plt.xlabel('Days of Data Available')\n",
    "plt.ylabel('Number of Combinations')\n",
    "plt.title('Distribution of Data Availability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Compare dataset sizes\n",
    "datasets = ['Logistic\\nRegression', 'XGBoost', 'LSTM']\n",
    "sizes = [2192, 2192, len(sequences)]\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars = plt.bar(datasets, sizes, color=colors, edgecolor='black')\n",
    "plt.ylabel('Number of Training Examples')\n",
    "plt.title('Dataset Size Comparison')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "             str(size), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset size impact:\")\n",
    "print(f\"LSTM has {len(sequences)/2192*100:.1f}% of the data available to other models\")\n",
    "print(f\"This significantly limits the LSTM's ability to learn complex patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three models\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'XGBoost', 'LSTM'],\n",
    "    'ROC-AUC': [0.9032, 0.9121, final_auc],\n",
    "    'Dataset Size': [2192, 2192, len(sequences)],\n",
    "    'Training Time': ['Fast', 'Medium', 'Slow'],\n",
    "    'Complexity': ['Low', 'Medium', 'High'],\n",
    "    'Feature Type': ['Engineered', 'Engineered + Advanced', 'Sequential + Static']\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Model Comparison Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize model performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# AUC comparison\n",
    "models = ['Logistic\\nRegression', 'XGBoost', 'LSTM']\n",
    "aucs = [0.9032, 0.9121, final_auc]\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "bars1 = ax1.bar(models, aucs, color=colors, edgecolor='black')\n",
    "ax1.set_ylabel('ROC-AUC Score')\n",
    "ax1.set_title('Model Performance Comparison')\n",
    "ax1.set_ylim(0.4, 1.0)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, auc in zip(bars1, aucs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{auc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Dataset size vs performance\n",
    "dataset_sizes = [2192, 2192, len(sequences)]\n",
    "ax2.scatter(dataset_sizes, aucs, c=colors, s=200, alpha=0.7, edgecolor='black')\n",
    "for i, model in enumerate(['LR', 'XGB', 'LSTM']):\n",
    "    ax2.annotate(model, (dataset_sizes[i], aucs[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontweight='bold')\n",
    "ax2.set_xlabel('Dataset Size (Training Examples)')\n",
    "ax2.set_ylabel('ROC-AUC Score')\n",
    "ax2.set_title('Dataset Size vs Performance')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and metadata\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save PyTorch model\n",
    "model_save_dict = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'sequence_input_size': sequence_input_size,\n",
    "        'static_input_size': static_input_size,\n",
    "        'hidden_size': 64,\n",
    "        'num_layers': 2,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    'sequence_length': sequence_length,\n",
    "    'final_auc': final_auc,\n",
    "    'best_auc': best_auc,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'test_aucs': test_aucs,\n",
    "    'num_sequences': len(sequences)\n",
    "}\n",
    "\n",
    "torch.save(model_save_dict, model_dir / 'lstm_model.pth')\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(train_dataset.scaler, model_dir / 'lstm_static_scaler.pkl')\n",
    "joblib.dump(train_dataset.sequence_scaler, model_dir / 'lstm_sequence_scaler.pkl')\n",
    "\n",
    "print(f\"Model saved to {model_dir}\")\n",
    "print(f\"Final ROC-AUC: {final_auc:.4f}\")\n",
    "print(f\"Best ROC-AUC during training: {best_auc:.4f}\")\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - lstm_model.pth (model + training history)\")\n",
    "print(f\"  - lstm_static_scaler.pkl (static feature scaler)\")\n",
    "print(f\"  - lstm_sequence_scaler.pkl (sequence scaler)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Analysis\n",
    "\n",
    "### LSTM Model Performance:\n",
    "- **ROC-AUC**: Significantly lower than other models due to data limitations\n",
    "- **Dataset Size**: Only 340 sequences vs 2,192 examples for other models\n",
    "- **Architecture**: Complex model with 58,113+ parameters\n",
    "- **Sequential Learning**: Captures temporal patterns but requires more data\n",
    "\n",
    "### Key Limitations:\n",
    "1. **Insufficient Sequential Data**: Only 15.5% of combinations have enough historical data\n",
    "2. **Small Dataset Size**: LSTM has 15.5% of the training data available to other models\n",
    "3. **Overfitting Risk**: Complex architecture with limited data leads to poor generalization\n",
    "4. **Temporal Requirements**: LSTM needs longer, consistent time series\n",
    "\n",
    "### When LSTM Would Excel:\n",
    "1. **More Historical Data**: 6+ months of consistent daily pricing data\n",
    "2. **Longer Sequences**: 30+ day lookback windows\n",
    "3. **Complex Temporal Patterns**: Seasonal cycles, weekly patterns, holiday effects\n",
    "4. **Larger Dataset**: 10,000+ sequences for proper training\n",
    "\n",
    "### Recommendations:\n",
    "1. **Use XGBoost for Production**: Best performance with current data constraints\n",
    "2. **Collect More Historical Data**: Enable LSTM training with longer sequences\n",
    "3. **Ensemble Approach**: Combine LSTM predictions with XGBoost for robustness\n",
    "4. **Feature Engineering**: Add more temporal features to traditional models\n",
    "\n",
    "The LSTM model demonstrates the importance of matching model complexity to data availability. While powerful for sequential learning, it requires substantially more data than traditional ML approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}